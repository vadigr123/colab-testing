{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLxoReNglNRsJgA0JgTc+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadigr123/colab-testing/blob/main/Downloader-gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vaGGnqUERpO3"
      },
      "outputs": [],
      "source": [
        "#@title ## üê∫ E621 & e926\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Paste URL e621\n",
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# üîπ User Settings\n",
        "search_url = \"https://e621.net/posts?tags=hug\"  # @param {type:\"string\"}\n",
        "max_images = 10  # @param {type:\"integer\"}\n",
        "include_gif_video = False  # @param {type:\"boolean\"}\n",
        "save_path = \"/content/e621_test_2\"  # @param {type:\"string\"}\n",
        "\n",
        "# üîπ API Settings\n",
        "BASE_URL = \"https://e621.net/posts.json\"\n",
        "HEADERS = {\"User-Agent\": \"MyColabScript/1.0 (by username on e621)\"}\n",
        "ALLOWED_FORMATS = [\"jpg\", \"jpeg\", \"png\"]\n",
        "\n",
        "if include_gif_video:\n",
        "    ALLOWED_FORMATS.extend([\"gif\", \"webm\", \"mp4\"])\n",
        "\n",
        "# Create download folder\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# üîç Extract tags from URL + Decoding\n",
        "def extract_tags_from_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "    raw_tags = query_params.get(\"tags\", [\"\"])[0]\n",
        "    decoded_tags = urllib.parse.unquote(raw_tags)  # URL decoding\n",
        "    return decoded_tags.replace(\"+\", \" \")  # Replace \"+\" with spaces\n",
        "\n",
        "tags = extract_tags_from_url(search_url)\n",
        "if not tags:\n",
        "    print(\"‚ö†Ô∏è Unable to find tags in URL!\")\n",
        "    exit()\n",
        "\n",
        "print(f\"üîç Searching by tags: {tags}\")\n",
        "\n",
        "# üîç Search for images via API (loading by pages)\n",
        "def fetch_images(tags, max_images=-1):\n",
        "    all_posts = []\n",
        "    page = 1\n",
        "\n",
        "    while len(all_posts) < max_images or max_images == -1:\n",
        "        params = {\"limit\": 320, \"tags\": tags, \"page\": page}  # Max 320 posts per request\n",
        "\n",
        "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ö†Ô∏è API Error {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "            posts = data.get(\"posts\", [])\n",
        "            if not posts:\n",
        "                break  # If no more images, exit the loop\n",
        "\n",
        "            all_posts.extend(posts)  # Add new posts to the list\n",
        "            page += 1  # Move to the next page\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è JSON Error: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_posts[:max_images] if max_images != -1 else all_posts  # Trim to the required limit\n",
        "\n",
        "# üì• Download files\n",
        "def download_images(posts, max_images=-1, include_gif_video=False):\n",
        "    downloaded = 0\n",
        "\n",
        "    for post in tqdm(posts, desc=\"üì• Downloading\", unit=\"files\"):\n",
        "        if max_images != -1 and downloaded >= max_images:\n",
        "            break\n",
        "\n",
        "        # Check for file URL presence\n",
        "        file_url = post.get(\"file\", {}).get(\"url\", \"\")\n",
        "        if not file_url:\n",
        "            continue  # If no file URL, skip this post\n",
        "\n",
        "        file_ext = file_url.split(\".\")[-1]\n",
        "\n",
        "        if file_ext not in ALLOWED_FORMATS and (not include_gif_video or file_ext not in ['gif', 'webm', 'mp4']):\n",
        "            continue\n",
        "\n",
        "        file_name = f\"{post['id']}.{file_ext}\"\n",
        "        file_path = os.path.join(save_path, file_name)\n",
        "\n",
        "        try:\n",
        "            img_data = requests.get(file_url, headers=HEADERS).content\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "            downloaded += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download Error {file_name}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Downloaded {downloaded} files.\")\n",
        "\n",
        "# üì¶ Create ZIP archive\n",
        "def create_zip():\n",
        "    zip_path = save_path + \".zip\"\n",
        "    shutil.make_archive(save_path, \"zip\", save_path)\n",
        "    return zip_path\n",
        "\n",
        "# üöÄ Execution\n",
        "posts = fetch_images(tags, max_images)\n",
        "if posts:\n",
        "    download_images(posts, max_images, include_gif_video)\n",
        "    zip_file = create_zip()\n",
        "    print(f\"üì¶ ZIP created: {zip_file}\")\n",
        "    files.download(zip_file)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No images found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0uSSmBmncVOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### ‚ñ∂Ô∏è Paste URL e926\n",
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# üîπ User Settings\n",
        "search_url = \"https://e926.net/posts?tags=hug\"  # @param {type:\"string\"}\n",
        "max_images = 10  # @param {type:\"integer\"}\n",
        "include_gif_video = False  # @param {type:\"boolean\"}\n",
        "save_path = \"/content/e926_test1\"  # @param {type:\"string\"}\n",
        "\n",
        "# üîπ API Settings\n",
        "BASE_URL = \"https://e926.net/posts.json\"\n",
        "HEADERS = {\"User-Agent\": \"MyColabScript/1.0 (by username on e926)\"}\n",
        "ALLOWED_FORMATS = [\"jpg\", \"jpeg\", \"png\"]\n",
        "\n",
        "if include_gif_video:\n",
        "    ALLOWED_FORMATS.extend([\"gif\", \"webm\", \"mp4\"])\n",
        "\n",
        "# Create download folder\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# üîç Extract tags from URL + Decoding\n",
        "def extract_tags_from_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "    raw_tags = query_params.get(\"tags\", [\"\"])[0]\n",
        "    decoded_tags = urllib.parse.unquote(raw_tags)  # URL decoding\n",
        "    return decoded_tags.replace(\"+\", \" \")  # Replace \"+\" with spaces\n",
        "\n",
        "tags = extract_tags_from_url(search_url)\n",
        "if not tags:\n",
        "    print(\"‚ö†Ô∏è Unable to find tags in URL!\")\n",
        "    exit()\n",
        "\n",
        "print(f\"üîç Searching by tags: {tags}\")\n",
        "\n",
        "# üîç Search for images via API (loading by pages)\n",
        "def fetch_images(tags, limit=1000):\n",
        "    all_posts = []\n",
        "    page = 1\n",
        "\n",
        "    while len(all_posts) < limit:\n",
        "        params = {\"limit\": 320, \"tags\": tags, \"page\": page}  # Max 320 posts per request\n",
        "\n",
        "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ö†Ô∏è API Error {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "            posts = data.get(\"posts\", [])\n",
        "            if not posts:\n",
        "                break  # If no more images, exit the loop\n",
        "\n",
        "            all_posts.extend(posts)  # Add new posts to the list\n",
        "            page += 1  # Move to the next page\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è JSON Error: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_posts[:limit]  # Trim to the required limit\n",
        "\n",
        "# üì• Download files\n",
        "def download_images(posts, max_images=-1):\n",
        "    downloaded = 0\n",
        "\n",
        "    for post in tqdm(posts, desc=\"üì• Downloading\", unit=\"files\"):\n",
        "        if max_images != -1 and downloaded >= max_images:\n",
        "            break\n",
        "\n",
        "        # Check for file URL presence\n",
        "        file_url = post.get(\"file\", {}).get(\"url\", \"\")\n",
        "        if not file_url:\n",
        "            continue  # If no file URL, skip this post\n",
        "\n",
        "        file_ext = file_url.split(\".\")[-1]\n",
        "\n",
        "        if file_ext not in ALLOWED_FORMATS:\n",
        "            continue\n",
        "\n",
        "        file_name = f\"{post['id']}.{file_ext}\"\n",
        "        file_path = os.path.join(save_path, file_name)\n",
        "\n",
        "        try:\n",
        "            img_data = requests.get(file_url, headers=HEADERS).content\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "            downloaded += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download Error {file_name}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Downloaded {downloaded} files.\")\n",
        "\n",
        "# üì¶ Create ZIP archive\n",
        "def create_zip():\n",
        "    zip_path = save_path + \".zip\"\n",
        "    shutil.make_archive(save_path, \"zip\", save_path)\n",
        "    return zip_path\n",
        "\n",
        "# üöÄ Execution\n",
        "posts = fetch_images(tags, max_images)\n",
        "if posts:\n",
        "    download_images(posts)\n",
        "    zip_file = create_zip()\n",
        "    print(f\"üì¶ ZIP created: {zip_file}\")\n",
        "    files.download(zip_file)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No images found!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QzfjTx8xZOXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "QW6gHCYkWnuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## üì¶ Danbooru\n",
        "\n",
        "#@markdown ### ‚ñ∂Ô∏è Paste URL danbooru\n",
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import urllib.parse\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# üîπ User Settings\n",
        "search_url = \"https://danbooru.donmai.us/posts?tags=hug\"  # @param {type:\"string\"}\n",
        "max_images = 10  # @param {type:\"integer\"}\n",
        "include_gif_video = False  # @param {type:\"boolean\"}\n",
        "save_path = \"/content/danbooru_test\"  # @param {type:\"string\"}\n",
        "\n",
        "# üîπ API Settings\n",
        "BASE_URL = \"https://danbooru.donmai.us/posts.json\"\n",
        "HEADERS = {\"User-Agent\": \"MyColabScript/1.0 (by username on Danbooru)\"}\n",
        "ALLOWED_FORMATS = [\"jpg\", \"jpeg\", \"png\"]\n",
        "\n",
        "if include_gif_video:\n",
        "    ALLOWED_FORMATS.extend([\"gif\", \"webm\", \"mp4\"])\n",
        "\n",
        "# Create download folder\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# üîç Extract tags from URL + Decoding\n",
        "def extract_tags_from_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "    raw_tags = query_params.get(\"tags\", [\"\"])[0]\n",
        "    decoded_tags = urllib.parse.unquote(raw_tags)  # URL decoding\n",
        "    return decoded_tags.replace(\"+\", \" \")  # Replace \"+\" with spaces\n",
        "\n",
        "tags = extract_tags_from_url(search_url)\n",
        "if not tags:\n",
        "    print(\"‚ö†Ô∏è Unable to find tags in URL!\")\n",
        "    exit()\n",
        "\n",
        "print(f\"üîç Searching by tags: {tags}\")\n",
        "\n",
        "# üîç Search for images via API (loading by pages)\n",
        "def fetch_images(tags, max_images=-1):\n",
        "    all_posts = []\n",
        "    page = 1\n",
        "\n",
        "    while len(all_posts) < max_images or max_images == -1:\n",
        "        params = {\"limit\": 320, \"tags\": tags, \"page\": page}  # Max 320 posts per request\n",
        "\n",
        "        response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ö†Ô∏è API Error {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "            posts = data\n",
        "            if not posts:\n",
        "                break  # If no more images, exit the loop\n",
        "\n",
        "            all_posts.extend(posts)  # Add new posts to the list\n",
        "            page += 1  # Move to the next page\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è JSON Error: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_posts[:max_images] if max_images != -1 else all_posts  # Trim to the required limit\n",
        "\n",
        "# üì• Download files\n",
        "def download_images(posts, max_images=-1, include_gif_video=False):\n",
        "    downloaded = 0\n",
        "\n",
        "    for post in tqdm(posts, desc=\"üì• Downloading\", unit=\"files\"):\n",
        "        if max_images != -1 and downloaded >= max_images:\n",
        "            break\n",
        "\n",
        "        # Check for file URL presence\n",
        "        file_url = post.get(\"file_url\", \"\")\n",
        "        if not file_url:\n",
        "            continue  # If no file URL, skip this post\n",
        "\n",
        "        file_ext = file_url.split(\".\")[-1]\n",
        "\n",
        "        if file_ext not in ALLOWED_FORMATS and (not include_gif_video or file_ext not in ['gif', 'webm', 'mp4']):\n",
        "            continue\n",
        "\n",
        "        file_name = f\"{post['id']}.{file_ext}\"\n",
        "        file_path = os.path.join(save_path, file_name)\n",
        "\n",
        "        try:\n",
        "            img_data = requests.get(file_url, headers=HEADERS).content\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "            downloaded += 1\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download Error {file_name}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Downloaded {downloaded} files.\")\n",
        "\n",
        "# üì¶ Create ZIP archive\n",
        "def create_zip():\n",
        "    zip_path = save_path + \".zip\"\n",
        "    shutil.make_archive(save_path, \"zip\", save_path)\n",
        "    return zip_path\n",
        "\n",
        "# üöÄ Execution\n",
        "posts = fetch_images(tags, max_images)\n",
        "if posts:\n",
        "    download_images(posts, max_images, include_gif_video)\n",
        "    zip_file = create_zip()\n",
        "    print(f\"üì¶ ZIP created: {zip_file}\")\n",
        "    files.download(zip_file)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No images found!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hXhmeEX_V4XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7vG5Nn94XWDS"
      }
    }
  ]
}